{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OiH6O3jkNbsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ItvL8k9NbpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urLGvswhNYwV",
        "outputId": "bdd998b4-cd1d-4426-e3d2-1189fba2a9ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1minfo:\u001b[0m downloading installer\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mprofile set to 'default'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdefault host triple is x86_64-unknown-linux-gnu\n",
            "\u001b[0m\u001b[1minfo: \u001b[0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mlatest update on 2025-04-03, rust version 1.86.0 (05f9846f8 2025-03-31)\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'cargo'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'clippy'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-docs'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-std'\n",
            " 27.1 MiB /  27.1 MiB (100 %)  19.0 MiB/s in  1s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustc'\n",
            " 72.8 MiB /  72.8 MiB (100 %)  36.7 MiB/s in  2s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustfmt'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'cargo'\n",
            "  8.8 MiB /   8.8 MiB (100 %)   6.8 MiB/s in  1s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'clippy'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-docs'\n",
            " 21.2 MiB /  21.2 MiB (100 %)   2.7 MiB/s in  8s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-std'\n",
            " 27.1 MiB /  27.1 MiB (100 %)   5.5 MiB/s in  4s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustc'\n",
            " 72.8 MiB /  72.8 MiB (100 %)   8.4 MiB/s in  9s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustfmt'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
            "\n",
            "  \u001b[0m\u001b[1m\u001b[0m\u001b[1m\u001b[32mstable-x86_64-unknown-linux-gnu installed\u001b[0m - rustc 1.86.0 (05f9846f8 2025-03-31)\n",
            "\n",
            "\u001b[0m\u001b[1m\n",
            "Rust is installed now. Great!\n",
            "\u001b[0m\n",
            "To get started you may need to restart your current shell.\n",
            "This would reload your \u001b[0m\u001b[1mPATH\u001b[0m environment variable to include\n",
            "Cargo's bin directory ($HOME/.cargo/bin).\n",
            "\n",
            "To configure your current shell, you need to source\n",
            "the corresponding \u001b[0m\u001b[1menv\u001b[0m file under $HOME/.cargo.\n",
            "\n",
            "This is usually done by running one of the following (note the leading DOT):\n",
            ". \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\n",
            "source \"$HOME/.cargo/env.fish\"  # For fish\n",
            "source $\"($nu.home-path)/.cargo/env.nu\"  # For nushell\n",
            "cargo 1.86.0 (adf9b6ad1 2025-02-28)\n"
          ]
        }
      ],
      "source": [
        "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
        "\n",
        "import os\n",
        "os.environ['PATH'] += \":/root/.cargo/bin\"\n",
        "\n",
        "!cargo --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/dustletter/llama-rs"
      ],
      "metadata": {
        "id": "n1JLjWDGNym0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/dustletter/llama-rs#user-content-fn-1-6ec8def240a21bd6d9b3c78391f00446"
      ],
      "metadata": {
        "id": "to_LdHu5Obmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dustletter/llama-rs.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9hKnDSUN-lb",
        "outputId": "2373f666-e0dc-4d82-b1f6-fbddd9ad06da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama-rs'...\n",
            "remote: Enumerating objects: 3311, done.\u001b[K\n",
            "remote: Total 3311 (delta 0), reused 0 (delta 0), pack-reused 3311 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3311/3311), 5.05 MiB | 30.62 MiB/s, done.\n",
            "Resolving deltas: 100% (1598/1598), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama-rs/llama-cli\n",
        "!cargo install --git https://github.com/rustformers/llama-rs llama-cli\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMNw9BjiNzMr",
        "outputId": "294314ed-103d-4d26-af3a-89120f9049d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'llama-rs/llama-cli'\n",
            "/content/llama-rs\n",
            "\u001b[1m\u001b[32m    Updating\u001b[0m git repository `https://github.com/rustformers/llama-rs`\n",
            "\u001b[1m\u001b[31merror\u001b[0m\u001b[1m:\u001b[0m could not find `llama-cli` in https://github.com/rustformers/llama-rs with version `*`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cargo install --git https://github.com/rustformers/llama-rs llama-cli"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MVlaSwLOEvr",
        "outputId": "2714947e-1634-42d4-8b94-4cd8e2774742"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Updating\u001b[0m git repository `https://github.com/rustformers/llama-rs`\n",
            "\u001b[1m\u001b[31merror\u001b[0m\u001b[1m:\u001b[0m could not find `llama-cli` in https://github.com/rustformers/llama-rs with version `*`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cargo build --release --bin llama-cli"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVqhKSQ6Oi4L",
        "outputId": "1a0c15a8-2f80-4edb-f130-ac9591181fa5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Updating\u001b[0m crates.io index\n",
            "\u001b[1m\u001b[32m Downloading\u001b[0m crates ...\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m addr2line v0.19.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m anstyle v0.3.5\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m anstream v0.2.6\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m autocfg v1.1.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m anstyle-parse v0.1.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m adler v1.0.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m ahash v0.3.8\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m zstd v0.12.3+zstd.1.5.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m strum v0.24.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m env_logger v0.10.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m which v4.4.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m zstd-safe v6.0.5+zstd.1.5.4\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m scopeguard v1.1.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m tinyvec_macros v0.1.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m tinyvec v1.6.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m unicode-ident v1.0.8\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m unicode-segmentation v1.10.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m regex v1.7.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m serde v1.0.159\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m syn v1.0.109\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m unicode-normalization-alignments v0.1.12\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rayon-core v1.11.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rayon v1.7.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m regex-syntax v0.6.29\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m minimal-lexical v0.2.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m memchr v2.5.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m radix_trie v0.2.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m syn v2.0.13\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m nix v0.26.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m spinners v4.1.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m csv v1.2.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m serde_json v1.0.95\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m linux-raw-sys v0.3.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m nom v7.1.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rustyline v11.0.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m itertools v0.8.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rand v0.8.5\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m zstd-sys v2.0.8+zstd.1.5.5\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m libc v0.2.141\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m unicode-normalization v0.1.22\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m gimli v0.27.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m object v0.30.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m hashbrown v0.7.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m crossbeam-channel v0.5.7\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m color-eyre v0.6.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rustix v0.37.8\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m indenter v0.3.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m dirs-next v2.0.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m protobuf v2.14.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rust_tokenizers v3.1.6\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m clap_builder v4.2.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m cexpr v0.6.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m bindgen v0.64.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m strum_macros v0.24.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m static_assertions v1.1.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m smallvec v1.10.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m shlex v1.1.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m serde_derive v1.0.159\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m serde_bytes v0.11.9\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rustversion v1.0.12\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rustc-demangle v0.1.23\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rand_core v0.6.4\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rand_chacha v0.3.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m thiserror-impl v1.0.40\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m thiserror v1.0.40\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m termcolor v1.2.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m ryu v1.0.13\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m quote v1.0.26\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m pkg-config v0.3.26\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m miniz_oxide v0.6.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m maplit v1.0.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m humantime v2.1.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m glob v0.3.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m crossbeam-utils v0.8.15\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m crossbeam-epoch v0.9.14\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m clang-sys v1.6.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m bytemuck v1.13.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m ppv-lite86 v0.2.17\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m peeking_take_while v0.1.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m owo-colors v3.5.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m num_cpus v1.15.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m nibble_vec v0.1.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m memoffset v0.8.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m memmap2 v0.5.10\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m libloading v0.7.4\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m lazycell v1.3.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m lazy_static v1.4.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m itoa v1.0.6\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m heck v0.4.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m endian-type v0.1.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m either v1.8.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m dirs-sys-next v0.1.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m crossbeam-deque v0.8.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m bincode v1.3.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m unicode-width v0.1.10\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m utf8parse v0.2.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m proc-macro2 v1.0.56\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m partial_sort v0.2.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m once_cell v1.17.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m log v0.4.17\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m jobserver v0.1.26\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m is-terminal v0.4.7\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m io-lifetimes v1.0.10\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m half v2.2.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m getrandom v0.2.9\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m fd-lock v3.0.12\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m eyre v0.6.8\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m csv-core v0.1.10\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m concolor-override v1.0.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m clap_lex v0.4.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m backtrace v0.3.67\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m strsim v0.10.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m concolor-query v0.3.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m clap_derive v4.2.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m clap v4.2.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m cc v1.0.79\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rustc-hash v1.1.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m cfg-if v1.0.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m bitflags v1.3.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m aho-corasick v0.7.20\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m libc v0.2.141\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m proc-macro2 v1.0.56\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m cfg-if v1.0.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m quote v1.0.26\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-ident v1.0.8\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m autocfg v1.1.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m syn v2.0.13\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m jobserver v0.1.26\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m cc v1.0.79\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m memchr v2.5.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m io-lifetimes v1.0.10\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m bitflags v1.3.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rustix v0.37.8\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-utils v0.8.15\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m serde_derive v1.0.159\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m memoffset v0.8.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m linux-raw-sys v0.3.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m serde v1.0.159\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-epoch v0.9.14\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m scopeguard v1.1.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m pkg-config v0.3.26\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m smallvec v1.10.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m zstd-sys v2.0.8+zstd.1.5.5\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ggml-sys v0.1.0 (/content/llama-rs/ggml-sys)\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rustversion v1.0.12\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m syn v1.0.109\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m utf8parse v0.2.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m heck v0.4.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rayon-core v1.11.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-deque v0.8.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-channel v0.5.7\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m is-terminal v0.4.7\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m aho-corasick v0.7.20\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m hashbrown v0.7.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m num_cpus v1.15.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m getrandom v0.2.9\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m serde_json v1.0.95\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m either v1.8.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m tinyvec_macros v0.1.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ryu v1.0.13\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m protobuf v2.14.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m log v0.4.17\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m thiserror v1.0.40\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m regex-syntax v0.6.29\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m itoa v1.0.6\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m tinyvec v1.6.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rand_core v0.6.4\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m regex v1.7.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m anstyle-parse v0.1.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m thiserror-impl v1.0.40\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m csv-core v0.1.10\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m backtrace v0.3.67\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m concolor-query v0.3.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ahash v0.3.8\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m concolor-override v1.0.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m eyre v0.6.8\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m anstyle v0.3.5\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m once_cell v1.17.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ppv-lite86 v0.2.17\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m zstd-safe v6.0.5+zstd.1.5.4\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rust_tokenizers v3.1.6\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m lazy_static v1.4.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m adler v1.0.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m gimli v0.27.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m miniz_oxide v0.6.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rand_chacha v0.3.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m anstream v0.2.6\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m addr2line v0.19.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m csv v1.2.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m strum_macros v0.24.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rayon v1.7.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-normalization v0.1.22\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m itertools v0.8.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m nibble_vec v0.1.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-normalization-alignments v0.1.12\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m object v0.30.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m dirs-sys-next v0.1.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m indenter v0.3.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m static_assertions v1.1.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m strsim v0.10.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rustc-demangle v0.1.23\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m endian-type v0.1.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m clap_lex v0.4.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m clap_builder v4.2.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m radix_trie v0.2.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m nix v0.26.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m dirs-next v2.0.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m strum v0.24.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rand v0.8.5\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m serde_bytes v0.11.9\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m clap_derive v4.2.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m fd-lock v3.0.12\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m memmap2 v0.5.10\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m owo-colors v3.5.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-width v0.1.10\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m partial_sort v0.2.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m maplit v1.0.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m humantime v2.1.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m termcolor v1.2.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m half v2.2.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-segmentation v1.10.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m bytemuck v1.13.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rustyline v11.0.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m env_logger v0.10.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m spinners v4.1.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m color-eyre v0.6.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m clap v4.2.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m bincode v1.3.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ggml v0.1.0 (/content/llama-rs/ggml)\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ggml-format v0.1.0 (/content/llama-rs/ggml-format)\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m llama-rs v0.1.0 (/content/llama-rs/llama-rs)\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m zstd v0.12.3+zstd.1.5.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m llama-cli v0.1.0 (/content/llama-rs/llama-cli)\n",
            "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 3m 46s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ3_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxRBc22rQDpr",
        "outputId": "785baa76-7922-4b89-9702-e652a6f88975"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2025-05-06 20:19:14--  https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-IQ3_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.90, 3.163.189.37, 3.163.189.74, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.90|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/66f457f561dbc064f4a6413f/5dda430a1ed394ec8000cf55484363a50aa3de08dbd386f768f6efdd39a74cc5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250506T201914Z&X-Amz-Expires=3600&X-Amz-Signature=97a4861707532a78113b2744f44fb04a3ea9f02330cf62c1ae9c022b5f003a84&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-3B-Instruct-IQ3_M.gguf%3B+filename%3D%22Llama-3.2-3B-Instruct-IQ3_M.gguf%22%3B&x-id=GetObject&Expires=1746566354&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NjU2NjM1NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmY0NTdmNTYxZGJjMDY0ZjRhNjQxM2YvNWRkYTQzMGExZWQzOTRlYzgwMDBjZjU1NDg0MzYzYTUwYWEzZGUwOGRiZDM4NmY3NjhmNmVmZGQzOWE3NGNjNSoifV19&Signature=IL-SaBJl46rMmAhXpKZ%7EwV9TJWk8mMK4kXTkTOF46Kt0pPC9PB5Rfr1%7EQRj5A7%7E30PufCARuhvBq4KPqjoC1mMoKNdtep2L8eiWXZItpmUOES4pLWZGDRVavYKHN%7EC8phiQRjEDVeIqybuHjZHJt9tmjftclEz3Jr4TZvxERwc8rRCi8FqZaqdMUSQxR9UuErOo2V92M0dCctQVSOIm2IAP21KjiGMPy%7EJkiog8EzJtHHAHO8lNOPhvIkuT0mHuj48eA-hCe9te3OHgkslXfdm2uVCvO4Ur7U3IVBcnOha911DWj4IvH6Bs97HbEg8WEgm2-b4XA78bpLlEMPnFqgQ__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-05-06 20:19:14--  https://cas-bridge.xethub.hf.co/xet-bridge-us/66f457f561dbc064f4a6413f/5dda430a1ed394ec8000cf55484363a50aa3de08dbd386f768f6efdd39a74cc5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250506T201914Z&X-Amz-Expires=3600&X-Amz-Signature=97a4861707532a78113b2744f44fb04a3ea9f02330cf62c1ae9c022b5f003a84&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-3B-Instruct-IQ3_M.gguf%3B+filename%3D%22Llama-3.2-3B-Instruct-IQ3_M.gguf%22%3B&x-id=GetObject&Expires=1746566354&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NjU2NjM1NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmY0NTdmNTYxZGJjMDY0ZjRhNjQxM2YvNWRkYTQzMGExZWQzOTRlYzgwMDBjZjU1NDg0MzYzYTUwYWEzZGUwOGRiZDM4NmY3NjhmNmVmZGQzOWE3NGNjNSoifV19&Signature=IL-SaBJl46rMmAhXpKZ%7EwV9TJWk8mMK4kXTkTOF46Kt0pPC9PB5Rfr1%7EQRj5A7%7E30PufCARuhvBq4KPqjoC1mMoKNdtep2L8eiWXZItpmUOES4pLWZGDRVavYKHN%7EC8phiQRjEDVeIqybuHjZHJt9tmjftclEz3Jr4TZvxERwc8rRCi8FqZaqdMUSQxR9UuErOo2V92M0dCctQVSOIm2IAP21KjiGMPy%7EJkiog8EzJtHHAHO8lNOPhvIkuT0mHuj48eA-hCe9te3OHgkslXfdm2uVCvO4Ur7U3IVBcnOha911DWj4IvH6Bs97HbEg8WEgm2-b4XA78bpLlEMPnFqgQ__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.64, 18.238.217.126, 18.238.217.88, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1599668768 (1.5G)\n",
            "Saving to: ‘Llama-3.2-3B-Instruct-IQ3_M.gguf’\n",
            "\n",
            "Llama-3.2-3B-Instru 100%[===================>]   1.49G  87.2MB/s    in 19s     \n",
            "\n",
            "2025-05-06 20:19:33 (80.2 MB/s) - ‘Llama-3.2-3B-Instruct-IQ3_M.gguf’ saved [1599668768/1599668768]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!cargo run --release --bin llama-cli -- <ARGS>"
      ],
      "metadata": {
        "id": "sFIE9iC3OonT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama-rs/target/release/llama-cli infer -m <path>/ggml-model-q4_0.bin -p \"Tell me how cool the Rust programming language is:\""
      ],
      "metadata": {
        "id": "R98qYk4UP4ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oicusZiFQaW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aUeHDEvKQaT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/llama-rs\n",
        "!cargo run --release -- -m /content/Llama-3.2-3B-Instruct-IQ3_M.gguf -p \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmdB_2z7QaRF",
        "outputId": "ee96bfe6-4806-49c1-e8de-d0db03b142c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama-rs\n",
            "\u001b[1m\u001b[31merror\u001b[0m\u001b[1m:\u001b[0m `cargo run` could not determine which binary to run. Use the `--bin` option to specify a binary, or the `default-run` manifest key.\n",
            "available binaries: generate-ggml-bindings, llama-cli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cargo run --release --bin llama-cli"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr_l9w6oQfzF",
        "outputId": "4283f6e4-b366-477a-c7d2-7355633916f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 0.17s\n",
            "\u001b[1m\u001b[32m     Running\u001b[0m `target/release/llama-cli`\n",
            "\u001b[1m\u001b[4mUsage:\u001b[0m \u001b[1mllama-cli\u001b[0m <COMMAND>\n",
            "\n",
            "\u001b[1m\u001b[4mCommands\u001b[0m\u001b[1m\u001b[4m:\u001b[0m\n",
            "  \u001b[1minfer\u001b[0m              Use a model to infer the next tokens in a sequence, and exit\n",
            "  \u001b[1mdump-tokens\u001b[0m        Dumps the prompt to console and exits, first as a comma-separated list of token IDs and then as a list of comma-separated string keys and token ID values\n",
            "  \u001b[1mrepl\u001b[0m               Use a model to interactively prompt it multiple times, while resetting the context between invocations\n",
            "  \u001b[1mchat-experimental\u001b[0m  Use a model to interactively generate tokens, and chat with it\n",
            "  \u001b[1mquantize\u001b[0m           Quantize a GGML model to 4-bit\n",
            "  \u001b[1mhelp\u001b[0m               Print this message or the help of the given subcommand(s)\n",
            "\n",
            "\u001b[1m\u001b[4mOptions:\u001b[0m\n",
            "  \u001b[1m-h\u001b[0m, \u001b[1m--help\u001b[0m     Print help\n",
            "  \u001b[1m-V\u001b[0m, \u001b[1m--version\u001b[0m  Print version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cargo run --release --bin llama-cli -- -m /content/Llama-3.2-3B-Instruct-IQ3_M.gguf -p \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEsZX8kgQszc",
        "outputId": "1633b274-1621-4576-b2f0-3876a48f2f1f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 0.14s\n",
            "\u001b[1m\u001b[32m     Running\u001b[0m `target/release/llama-cli -m /content/Llama-3.2-3B-Instruct-IQ3_M.gguf -p hi`\n",
            "\u001b[1m\u001b[31merror:\u001b[0m unexpected argument '\u001b[33m-m\u001b[0m' found\n",
            "\n",
            "\u001b[1m\u001b[4mUsage:\u001b[0m \u001b[1mllama-cli\u001b[0m <COMMAND>\n",
            "\n",
            "For more information, try '\u001b[1m--help\u001b[0m'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cargo run --release --bin llama-cli chat-experimental --model-path /content/Llama-3.2-3B-Instruct-IQ3_M.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pntqewNZQ52k",
        "outputId": "65d97479-f180-4e4b-b4a3-07f896d24e76"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 0.20s\n",
            "\u001b[1m\u001b[32m     Running\u001b[0m `target/release/llama-cli chat-experimental --model-path /content/Llama-3.2-3B-Instruct-IQ3_M.gguf`\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:25:11Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loading model part 1/1 from '/content/Llama-3.2-3B-Instruct-IQ3_M.gguf' (mmap preferred: true)\n",
            "    \n",
            "Error: \n",
            "   0: \u001b[91mCould not load model\u001b[0m\n",
            "   1: \u001b[91minvalid magic number for \"/content/Llama-3.2-3B-Instruct-IQ3_M.gguf\"\u001b[0m\n",
            "\n",
            "Backtrace omitted. Run with RUST_BACKTRACE=1 environment variable to display it.\n",
            "Run with RUST_BACKTRACE=full to include source snippets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9bBM-NvvRczd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!cargo run --release --bin llama-cli chat-experimental --model-path /content/Llama-3.2-3B-Instruct-IQ3_M.gguf --gguf"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz0FWv7bRuYI",
        "outputId": "b5ffa9d6-753b-43e3-8163-cd922ab7fb2c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 0.21s\n",
            "\u001b[1m\u001b[32m     Running\u001b[0m `target/release/llama-cli chat-experimental --model-path /content/Llama-3.2-3B-Instruct-IQ3_M.gguf --gguf`\n",
            "\u001b[1m\u001b[31merror:\u001b[0m unexpected argument '\u001b[33m--gguf\u001b[0m' found\n",
            "\n",
            "\u001b[1m\u001b[4mUsage:\u001b[0m \u001b[1mllama-cli chat-experimental\u001b[0m <--model-path <MODEL_PATH>|--num-ctx-tokens <NUM_CTX_TOKENS>|--no-mmap>\n",
            "\n",
            "For more information, try '\u001b[1m--help\u001b[0m'.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!cargo run --release --bin llama-cli infer --model-path /content/Llama-3.2-3B-Instruct-IQ3_M.gguf --gguf -p \"Your prompt here\""
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5HB57_dR2lN",
        "outputId": "e67b4276-6716-465f-fbf9-994ff5f40020"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 0.20s\n",
            "\u001b[1m\u001b[32m     Running\u001b[0m `target/release/llama-cli infer --model-path /content/Llama-3.2-3B-Instruct-IQ3_M.gguf --gguf -p 'Your prompt here'`\n",
            "\u001b[1m\u001b[31merror:\u001b[0m unexpected argument '\u001b[33m--gguf\u001b[0m' found\n",
            "\n",
            "\u001b[1m\u001b[4mUsage:\u001b[0m \u001b[1mllama-cli infer\u001b[0m <--model-path <MODEL_PATH>|--num-ctx-tokens <NUM_CTX_TOKENS>|--no-mmap>\n",
            "\n",
            "For more information, try '\u001b[1m--help\u001b[0m'.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!cargo run --release --bin llama-cli convert --model-path /content/Llama-3.2-3B-Instruct-IQ3_M.gguf --output-path /content/Llama-3.2-3B-Instruct-IQ3_M.bin"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDpB0ZqdR988",
        "outputId": "4bd0b30c-0e3b-4183-c4ab-65fd86f3b863"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 0.14s\n",
            "\u001b[1m\u001b[32m     Running\u001b[0m `target/release/llama-cli convert --model-path /content/Llama-3.2-3B-Instruct-IQ3_M.gguf --output-path /content/Llama-3.2-3B-Instruct-IQ3_M.bin`\n",
            "\u001b[1m\u001b[31merror:\u001b[0m unexpected argument '\u001b[33m--model-path\u001b[0m' found\n",
            "\n",
            "\u001b[1m\u001b[4mUsage:\u001b[0m \u001b[1mllama-cli convert\u001b[0m [OPTIONS] \u001b[1m--\u001b[0m\u001b[1mdirectory\u001b[0m <DIRECTORY>\n",
            "\n",
            "For more information, try '\u001b[1m--help\u001b[0m'.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!cargo run --release --bin llama-cli infer --model-path /content/Llama-3.2-3B-Instruct-IQ3_M.bin -p \"Your prompt here\""
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H5LhxkiR_p1",
        "outputId": "fd5c4373-8078-42f6-d407-c7492868a007"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 0.11s\n",
            "\u001b[1m\u001b[32m     Running\u001b[0m `target/release/llama-cli infer --model-path /content/Llama-3.2-3B-Instruct-IQ3_M.bin -p 'Your prompt here'`\n",
            "Error: \n",
            "   0: \u001b[91mCould not load model\u001b[0m\n",
            "   1: \u001b[91mmultipart models are not supported\u001b[0m\n",
            "\n",
            "Backtrace omitted. Run with RUST_BACKTRACE=1 environment variable to display it.\n",
            "Run with RUST_BACKTRACE=full to include source snippets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget https://huggingface.co/Hamsa/ggml-alpaca-7b-q4.bin/resolve/main/ggml-alpaca-7b-q4.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kuemiTlVICG",
        "outputId": "96fe2d80-4a6b-4ad9-9cca-d7cc3d134242"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2025-05-06 20:41:14--  https://huggingface.co/Hamsa/ggml-alpaca-7b-q4.bin/resolve/main/ggml-alpaca-7b-q4.bin\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.90, 3.163.189.37, 3.163.189.74, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.90|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/19/c7/19c786e77efbd80752df9f137d06664f5279e23e40d04eb0afdf0905e944dc97/9c1bb4808f40aa0059d5343d3aac05fb75d368c240b664878d53d16bf27ade2b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ggml-alpaca-7b-q4.bin%3B+filename%3D%22ggml-alpaca-7b-q4.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1746567674&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NjU2NzY3NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8xOS9jNy8xOWM3ODZlNzdlZmJkODA3NTJkZjlmMTM3ZDA2NjY0ZjUyNzllMjNlNDBkMDRlYjBhZmRmMDkwNWU5NDRkYzk3LzljMWJiNDgwOGY0MGFhMDA1OWQ1MzQzZDNhYWMwNWZiNzVkMzY4YzI0MGI2NjQ4NzhkNTNkMTZiZjI3YWRlMmI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=YNPvkOeyaqMQZpVe-5EeHDeH8Wxp0x0fyKcXT-bcauFMDmdi0EgI8dbMrYPtYbVyltTMKO5ScCJnNUSXWUMF6at%7E4WxlhjX3dheTvePfxuo1CnKI6nKgGPDijdEvK23m157to9PfTIE0nOkms5jI3%7EhS4Fw31kOfw0a4WHG8gKNemVQjGHsOeqQPRPzZiOjyeKqZ1%7ErPBDhPonTQfXqopqU80kVu0m6P3oCkOneYYVX7xVtgZLqNoAGsYjCScdeen7%7EhmexkJchNKzzcle3vMLkucnosO0oQ8hrI-LbbVVBqYE-Wars0RUS%7Er723u4QENqI9PQjN%7ErpiQlJk9TiqRw__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-05-06 20:41:14--  https://cdn-lfs.hf.co/repos/19/c7/19c786e77efbd80752df9f137d06664f5279e23e40d04eb0afdf0905e944dc97/9c1bb4808f40aa0059d5343d3aac05fb75d368c240b664878d53d16bf27ade2b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ggml-alpaca-7b-q4.bin%3B+filename%3D%22ggml-alpaca-7b-q4.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1746567674&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NjU2NzY3NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8xOS9jNy8xOWM3ODZlNzdlZmJkODA3NTJkZjlmMTM3ZDA2NjY0ZjUyNzllMjNlNDBkMDRlYjBhZmRmMDkwNWU5NDRkYzk3LzljMWJiNDgwOGY0MGFhMDA1OWQ1MzQzZDNhYWMwNWZiNzVkMzY4YzI0MGI2NjQ4NzhkNTNkMTZiZjI3YWRlMmI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=YNPvkOeyaqMQZpVe-5EeHDeH8Wxp0x0fyKcXT-bcauFMDmdi0EgI8dbMrYPtYbVyltTMKO5ScCJnNUSXWUMF6at%7E4WxlhjX3dheTvePfxuo1CnKI6nKgGPDijdEvK23m157to9PfTIE0nOkms5jI3%7EhS4Fw31kOfw0a4WHG8gKNemVQjGHsOeqQPRPzZiOjyeKqZ1%7ErPBDhPonTQfXqopqU80kVu0m6P3oCkOneYYVX7xVtgZLqNoAGsYjCScdeen7%7EhmexkJchNKzzcle3vMLkucnosO0oQ8hrI-LbbVVBqYE-Wars0RUS%7Er723u4QENqI9PQjN%7ErpiQlJk9TiqRw__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.238.217.113, 18.238.217.63, 18.238.217.81, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.238.217.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4212727017 (3.9G) [application/octet-stream]\n",
            "Saving to: ‘ggml-alpaca-7b-q4.bin’\n",
            "\n",
            "ggml-alpaca-7b-q4.b 100%[===================>]   3.92G  45.1MB/s    in 89s     \n",
            "\n",
            "2025-05-06 20:42:43 (45.2 MB/s) - ‘ggml-alpaca-7b-q4.bin’ saved [4212727017/4212727017]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/llama-rs\n",
        "!cargo run --release  --bin llama-cli"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj3-Aw4PVMkW",
        "outputId": "a9d01285-38c0-4a86-c1f0-c51c1bdd58d4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama-rs\n",
            "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 0.16s\n",
            "\u001b[1m\u001b[32m     Running\u001b[0m `target/release/llama-cli`\n",
            "\u001b[1m\u001b[4mUsage:\u001b[0m \u001b[1mllama-cli\u001b[0m <COMMAND>\n",
            "\n",
            "\u001b[1m\u001b[4mCommands\u001b[0m\u001b[1m\u001b[4m:\u001b[0m\n",
            "  \u001b[1minfer\u001b[0m              Use a model to infer the next tokens in a sequence, and exit\n",
            "  \u001b[1mdump-tokens\u001b[0m        Dumps the prompt to console and exits, first as a comma-separated list of token IDs and then as a list of comma-separated string keys and token ID values\n",
            "  \u001b[1mrepl\u001b[0m               Use a model to interactively prompt it multiple times, while resetting the context between invocations\n",
            "  \u001b[1mchat-experimental\u001b[0m  Use a model to interactively generate tokens, and chat with it\n",
            "  \u001b[1mquantize\u001b[0m           Quantize a GGML model to 4-bit\n",
            "  \u001b[1mhelp\u001b[0m               Print this message or the help of the given subcommand(s)\n",
            "\n",
            "\u001b[1m\u001b[4mOptions:\u001b[0m\n",
            "  \u001b[1m-h\u001b[0m, \u001b[1m--help\u001b[0m     Print help\n",
            "  \u001b[1m-V\u001b[0m, \u001b[1m--version\u001b[0m  Print version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/llama-rs\n",
        "!cargo run --release  --bin llama-cli -- -m \"/content/ggml-alpaca-7b-q4.bin\" -p \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcbBpS4aVm2e",
        "outputId": "a3321a9b-4de1-4e88-a14b-3a8fd9088c4d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama-rs\n",
            "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 0.11s\n",
            "\u001b[1m\u001b[32m     Running\u001b[0m `target/release/llama-cli -m /content/ggml-alpaca-7b-q4.bin -p hi`\n",
            "\u001b[1m\u001b[31merror:\u001b[0m unexpected argument '\u001b[33m-m\u001b[0m' found\n",
            "\n",
            "\u001b[1m\u001b[4mUsage:\u001b[0m \u001b[1mllama-cli\u001b[0m <COMMAND>\n",
            "\n",
            "For more information, try '\u001b[1m--help\u001b[0m'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BmTu50YMV6hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama-rs/target/release/llama-cli infer -m /content/ggml-alpaca-7b-q4.bin -p \"Tell me how cool the Rust programming language is:\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRiSmYltWQau",
        "outputId": "6771d1fd-96b1-4cf6-e55b-a9a923656a03"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:30Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loading model part 1/1 from '/content/ggml-alpaca-7b-q4.bin' (mmap preferred: true)\n",
            "    \n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:31Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m ggml ctx size = 4017.32 MB\n",
            "    \n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:33Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 8/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:34Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 16/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:36Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 24/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:37Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 32/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:39Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 40/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:40Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 48/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:41Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 56/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:42Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 64/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:43Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 72/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:44Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 80/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:45Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 88/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:45Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 96/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:46Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 104/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:46Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 112/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:46Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 120/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:46Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 128/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:46Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 136/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:46Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 144/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:46Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 152/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:46Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 160/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:47Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 168/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:47Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 176/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:47Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 184/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:47Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 192/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:47Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 200/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:47Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 208/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:47Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 216/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:47Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 224/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:48Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 232/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:48Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 240/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:48Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 248/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:48Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 256/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:48Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 264/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:48Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 272/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:48Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 280/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:49Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 288/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:49Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loading of '/content/ggml-alpaca-7b-q4.bin' complete\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:49Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Model size = 0.00 MB / num tensors = 291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T20:46:49Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Model fully loaded! Elapsed: 18442ms\n",
            "Tell me how cool the Rust programming language is:\n",
            "Rust is a modern, open-source programming language created by Mozilla. It combines low-level performance with high-level safety and has an emphasis on memory management to provide superior stability for multi-threaded applications. Rust also offers exceptionally fast compilation times, allowing developers to quickly iterate on their projects. In addition, it boasts a large library of cross-platform tools that make development easier than ever before.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cargo run --release --bin llama-cli -- -m /content/ggml-alpaca-7b-q4.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_60nKmWSWZ0O",
        "outputId": "1c1bb564-fd43-43bc-c666-dab784826bc1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 0.23s\n",
            "\u001b[1m\u001b[32m     Running\u001b[0m `target/release/llama-cli -m /content/ggml-alpaca-7b-q4.bin`\n",
            "\u001b[1m\u001b[31merror:\u001b[0m unexpected argument '\u001b[33m-m\u001b[0m' found\n",
            "\n",
            "\u001b[1m\u001b[4mUsage:\u001b[0m \u001b[1mllama-cli\u001b[0m <COMMAND>\n",
            "\n",
            "For more information, try '\u001b[1m--help\u001b[0m'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cargo run --release --bin llama-cli --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JZ9k7BvXBfO",
        "outputId": "bcae448e-769d-4827-d94a-23221051d7c1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run a binary or example of the local package\n",
            "\n",
            "\u001b[1m\u001b[32mUsage:\u001b[0m \u001b[1m\u001b[36mcargo run\u001b[0m \u001b[36m[OPTIONS]\u001b[0m \u001b[36m[ARGS]...\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[32mArguments:\u001b[0m\n",
            "  \u001b[36m[ARGS]...\u001b[0m  Arguments for the binary or example to run\n",
            "\n",
            "\u001b[1m\u001b[32mOptions:\u001b[0m\n",
            "      \u001b[1m\u001b[36m--message-format\u001b[0m\u001b[36m \u001b[0m\u001b[36m<FMT>\u001b[0m     Error format\n",
            "  \u001b[1m\u001b[36m-v\u001b[0m, \u001b[1m\u001b[36m--verbose\u001b[0m\u001b[36m...\u001b[0m               Use verbose output (-vv very verbose/build.rs output)\n",
            "  \u001b[1m\u001b[36m-q\u001b[0m, \u001b[1m\u001b[36m--quiet\u001b[0m                    Do not print cargo log messages\n",
            "      \u001b[1m\u001b[36m--color\u001b[0m\u001b[36m \u001b[0m\u001b[36m<WHEN>\u001b[0m             Coloring: auto, always, never\n",
            "      \u001b[1m\u001b[36m--config\u001b[0m\u001b[36m \u001b[0m\u001b[36m<KEY=VALUE|PATH>\u001b[0m  Override a configuration value\n",
            "  \u001b[1m\u001b[36m-Z\u001b[0m\u001b[36m \u001b[0m\u001b[36m<FLAG>\u001b[0m                      Unstable (nightly-only) flags to Cargo, see 'cargo -Z help' for\n",
            "                                 details\n",
            "  \u001b[1m\u001b[36m-h\u001b[0m, \u001b[1m\u001b[36m--help\u001b[0m                     Print help\n",
            "\n",
            "\u001b[1m\u001b[32mPackage Selection:\u001b[0m\n",
            "  \u001b[1m\u001b[36m-p\u001b[0m, \u001b[1m\u001b[36m--package\u001b[0m\u001b[36m [\u001b[0m\u001b[36m<SPEC>\u001b[0m\u001b[36m]\u001b[0m  Package with the target to run\n",
            "\n",
            "\u001b[1m\u001b[32mTarget Selection:\u001b[0m\n",
            "      \u001b[1m\u001b[36m--bin\u001b[0m\u001b[36m [\u001b[0m\u001b[36m<NAME>\u001b[0m\u001b[36m]\u001b[0m      Name of the bin target to run\n",
            "      \u001b[1m\u001b[36m--example\u001b[0m\u001b[36m [\u001b[0m\u001b[36m<NAME>\u001b[0m\u001b[36m]\u001b[0m  Name of the example target to run\n",
            "\n",
            "\u001b[1m\u001b[32mFeature Selection:\u001b[0m\n",
            "  \u001b[1m\u001b[36m-F\u001b[0m, \u001b[1m\u001b[36m--features\u001b[0m\u001b[36m \u001b[0m\u001b[36m<FEATURES>\u001b[0m  Space or comma separated list of features to activate\n",
            "      \u001b[1m\u001b[36m--all-features\u001b[0m         Activate all available features\n",
            "      \u001b[1m\u001b[36m--no-default-features\u001b[0m  Do not activate the `default` feature\n",
            "\n",
            "\u001b[1m\u001b[32mCompilation Options:\u001b[0m\n",
            "  \u001b[1m\u001b[36m-j\u001b[0m, \u001b[1m\u001b[36m--jobs\u001b[0m\u001b[36m \u001b[0m\u001b[36m<N>\u001b[0m                Number of parallel jobs, defaults to # of CPUs.\n",
            "      \u001b[1m\u001b[36m--keep-going\u001b[0m              Do not abort the build as soon as there is an error\n",
            "  \u001b[1m\u001b[36m-r\u001b[0m, \u001b[1m\u001b[36m--release\u001b[0m                 Build artifacts in release mode, with optimizations\n",
            "      \u001b[1m\u001b[36m--profile\u001b[0m\u001b[36m \u001b[0m\u001b[36m<PROFILE-NAME>\u001b[0m  Build artifacts with the specified profile\n",
            "      \u001b[1m\u001b[36m--target\u001b[0m\u001b[36m [\u001b[0m\u001b[36m<TRIPLE>\u001b[0m\u001b[36m]\u001b[0m       Build for the target triple\n",
            "      \u001b[1m\u001b[36m--target-dir\u001b[0m\u001b[36m \u001b[0m\u001b[36m<DIRECTORY>\u001b[0m  Directory for all generated artifacts\n",
            "      \u001b[1m\u001b[36m--unit-graph\u001b[0m              Output build graph in JSON (unstable)\n",
            "      \u001b[1m\u001b[36m--timings\u001b[0m\u001b[36m[=\u001b[0m\u001b[36m<FMTS>\u001b[0m\u001b[36m]\u001b[0m        Timing output formats (unstable) (comma separated): html, json\n",
            "\n",
            "\u001b[1m\u001b[32mManifest Options:\u001b[0m\n",
            "      \u001b[1m\u001b[36m--manifest-path\u001b[0m\u001b[36m \u001b[0m\u001b[36m<PATH>\u001b[0m  Path to Cargo.toml\n",
            "      \u001b[1m\u001b[36m--lockfile-path\u001b[0m\u001b[36m \u001b[0m\u001b[36m<PATH>\u001b[0m  Path to Cargo.lock (unstable)\n",
            "      \u001b[1m\u001b[36m--ignore-rust-version\u001b[0m   Ignore `rust-version` specification in packages\n",
            "      \u001b[1m\u001b[36m--locked\u001b[0m                Assert that `Cargo.lock` will remain unchanged\n",
            "      \u001b[1m\u001b[36m--offline\u001b[0m               Run without accessing the network\n",
            "      \u001b[1m\u001b[36m--frozen\u001b[0m                Equivalent to specifying both --locked and --offline\n",
            "\n",
            "Run `\u001b[36m\u001b[1mcargo help run\u001b[39m\u001b[22m` for more detailed information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cargo help run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBzknY0zXI3-",
        "outputId": "8829a1d4-9677-4ad2-c3ee-75def50b0143"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This system has been minimized by removing packages and content that are\n",
            "not required on a system that users do not log into.\n",
            "\n",
            "To restore this content, including manpages, you can run the 'unminimize'\n",
            "command. You will still need to ensure the 'man-db' package is installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cargo run --release -- -m /content/ggml-alpaca-7b-q4.bin -p \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvHvLESAXZdG",
        "outputId": "a70fd728-91da-4f93-ab2e-f3e92556afde"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[31merror\u001b[0m\u001b[1m:\u001b[0m `cargo run` could not determine which binary to run. Use the `--bin` option to specify a binary, or the `default-run` manifest key.\n",
            "available binaries: generate-ggml-bindings, llama-cli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/llama-rs/generate-ggml-bindings\n",
        "!cargo run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl379yGuXtDe",
        "outputId": "3c3209ba-9e80-4b9c-b536-c49641e9fed3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama-rs/generate-ggml-bindings\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m memchr v2.5.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m glob v0.3.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m libc v0.2.141\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m proc-macro2 v1.0.56\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m clang-sys v1.6.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-ident v1.0.8\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m quote v1.0.26\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m cfg-if v1.0.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m minimal-lexical v0.2.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m log v0.4.17\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m syn v1.0.109\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m nom v7.1.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m libloading v0.7.4\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m either v1.8.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m bindgen v0.64.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m regex-syntax v0.6.29\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m cexpr v0.6.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m which v4.4.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m regex v1.7.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m shlex v1.1.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m peeking_take_while v0.1.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m lazy_static v1.4.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m bitflags v1.3.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rustc-hash v1.1.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m lazycell v1.3.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m generate-ggml-bindings v0.1.0 (/content/llama-rs/generate-ggml-bindings)\n",
            "\u001b[1m\u001b[32m    Finished\u001b[0m `dev` profile [unoptimized + debuginfo] target(s) in 36.63s\n",
            "\u001b[1m\u001b[32m     Running\u001b[0m `/content/llama-rs/target/debug/generate-ggml-bindings`\n",
            "Usage: /content/llama-rs/target/debug/generate-ggml-bindings <path_to_ggml_crate>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cargo install --git https://github.com/rustformers/llama-rs llama-cli"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYfKLuVvYPg_",
        "outputId": "5edd958b-3b4d-48c9-a284-a0d25ab696ef"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Updating\u001b[0m git repository `https://github.com/rustformers/llama-rs`\n",
            "\u001b[1m\u001b[31merror\u001b[0m\u001b[1m:\u001b[0m could not find `llama-cli` in https://github.com/rustformers/llama-rs with version `*`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/rustformers/llm.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WeGcuyIYkGn",
        "outputId": "e47f2c00-c69e-49cc-b026-ecd6dd4515db"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'llm'...\n",
            "remote: Enumerating objects: 10402, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 10402 (delta 0), reused 1 (delta 0), pack-reused 10399 (from 1)\u001b[K\n",
            "Receiving objects: 100% (10402/10402), 6.49 MiB | 20.91 MiB/s, done.\n",
            "Resolving deltas: 100% (5049/5049), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!cargo install --git https://github.com/rustformers/llama-rs llama-cli"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZpvTTiSZXpv",
        "outputId": "52c0b25f-69f2-45f7-80c9-0f92be7829f6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Updating\u001b[0m git repository `https://github.com/rustformers/llama-rs`\n",
            "\u001b[1m\u001b[31merror\u001b[0m\u001b[1m:\u001b[0m could not find `llama-cli` in https://github.com/rustformers/llama-rs with version `*`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-gjOnvGbZfXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!cargo install --git https://github.com/rustformers/llama-rs llama-cli"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0LW3XaZaT2X",
        "outputId": "4c89a2e4-99ca-456d-ad2e-94fd3368ea99"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Updating\u001b[0m git repository `https://github.com/rustformers/llama-rs`\n",
            "\u001b[1m\u001b[31merror\u001b[0m\u001b[1m:\u001b[0m could not find `llama-cli` in https://github.com/rustformers/llama-rs with version `*`\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!llama-cli --help"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb_QumM1aXa4",
        "outputId": "66e7c5a6-4745-4b21-a26a-7f627597ecf1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: llama-cli: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "XVE5FOrhc4uQ"
      }
    },
    {
      "source": [
        "!cargo install --git https://github.com/dustletter/llama-rs llama-cli"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA8tApAAabcV",
        "outputId": "afd77c39-8f93-4b2d-faf8-d7e71a40ba74"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[32m    Updating\u001b[0m git repository `https://github.com/dustletter/llama-rs`\n",
            "\u001b[1m\u001b[32m  Installing\u001b[0m llama-cli v0.1.0 (https://github.com/dustletter/llama-rs#6e8aa794)\n",
            "\u001b[1m\u001b[32m    Updating\u001b[0m crates.io index\n",
            "\u001b[1m\u001b[32m     Locking\u001b[0m 149 packages to latest compatible versions\n",
            "\u001b[1m\u001b[36m      Adding\u001b[0m bincode v1.3.3 \u001b[1m\u001b[33m(available: v2.0.1)\u001b[0m\n",
            "\u001b[1m\u001b[36m      Adding\u001b[0m env_logger v0.10.2 \u001b[1m\u001b[33m(available: v0.11.8)\u001b[0m\n",
            "\u001b[1m\u001b[36m      Adding\u001b[0m memmap2 v0.5.10 \u001b[1m\u001b[33m(available: v0.9.5)\u001b[0m\n",
            "\u001b[1m\u001b[36m      Adding\u001b[0m partial_sort v0.2.0 \u001b[1m\u001b[33m(available: v1.0.0)\u001b[0m\n",
            "\u001b[1m\u001b[36m      Adding\u001b[0m protobuf v2.14.0 \u001b[1m\u001b[33m(available: v2.28.0)\u001b[0m\n",
            "\u001b[1m\u001b[36m      Adding\u001b[0m rand v0.8.5 \u001b[1m\u001b[33m(available: v0.9.1)\u001b[0m\n",
            "\u001b[1m\u001b[36m      Adding\u001b[0m rust_tokenizers v3.1.6 \u001b[1m\u001b[33m(available: v8.1.1)\u001b[0m\n",
            "\u001b[1m\u001b[36m      Adding\u001b[0m rustyline v11.0.0 \u001b[1m\u001b[33m(available: v15.0.0)\u001b[0m\n",
            "\u001b[1m\u001b[36m      Adding\u001b[0m thiserror v1.0.69 \u001b[1m\u001b[33m(available: v2.0.12)\u001b[0m\n",
            "\u001b[1m\u001b[36m      Adding\u001b[0m zstd v0.12.4 \u001b[1m\u001b[33m(available: v0.13.3)\u001b[0m\n",
            "\u001b[1m\u001b[32m Downloading\u001b[0m crates ...\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m anstyle-query v1.1.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m anstyle v1.0.10\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m autocfg v1.4.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m anstream v0.6.18\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m addr2line v0.24.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m fd-lock v3.0.13\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m thiserror v1.0.69\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m zstd-safe v6.0.6\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m utf8parse v0.2.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m serde_bytes v0.11.17\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m zstd v0.12.4\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m unicode-ident v1.0.18\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m tinyvec v1.9.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m zerocopy v0.8.25\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m syn v2.0.101\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m nix v0.26.4\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m object v0.36.7\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m color-eyre v0.6.4\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m gimli v0.31.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m csv v1.3.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m libc v0.2.172\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rayon-core v1.12.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m quote v1.0.40\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m proc-macro2 v1.0.95\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m pkg-config v0.3.32\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m serde v1.0.219\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rustix v0.38.44\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rayon v1.10.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m miniz_oxide v0.8.8\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m unicode-normalization v0.1.24\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m memchr v2.7.4\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m regex v1.11.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m unicode-width v0.1.14\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m regex-syntax v0.8.5\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m linux-raw-sys v0.4.15\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m unicode-segmentation v1.12.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m owo-colors v4.2.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m serde_json v1.0.140\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m lazy_static v1.5.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m thiserror-impl v1.0.69\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m termcolor v1.4.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m strsim v0.11.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m smallvec v1.15.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m shlex v1.3.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m serde_derive v1.0.219\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m ryu v1.0.20\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rustversion v1.0.20\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rustc-demangle v0.1.24\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m ppv-lite86 v0.2.21\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m once_cell v1.21.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m num_cpus v1.16.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m is-terminal v0.4.16\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m eyre v0.6.12\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m errno v0.3.11\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m either v1.15.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m log v0.4.27\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m jobserver v0.1.33\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m is_terminal_polyfill v1.70.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m humantime v2.2.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m heck v0.5.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m half v2.6.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m env_logger v0.10.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m crossbeam-utils v0.8.21\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m crossbeam-epoch v0.9.18\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m clap_builder v4.5.37\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m regex-automata v0.4.9\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m clap v4.5.37\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m zstd-sys v2.0.15+zstd.1.5.7\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m cc v1.2.21\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m spinners v4.1.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m itoa v1.0.15\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m getrandom v0.2.16\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m csv-core v0.1.12\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m crossbeam-deque v0.8.6\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m colorchoice v1.0.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m bytemuck v1.23.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m bitflags v2.9.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m clap_lex v0.7.4\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m clap_derive v4.5.32\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m backtrace v0.3.75\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m aho-corasick v1.1.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m anstyle-parse v0.2.6\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m adler2 v2.0.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m proc-macro2 v1.0.95\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-ident v1.0.18\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m libc v0.2.172\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m quote v1.0.40\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m syn v2.0.101\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m jobserver v0.1.33\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m memchr v2.7.4\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m shlex v1.3.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m cc v1.2.21\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m cfg-if v1.0.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-utils v0.8.21\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m serde v1.0.219\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m serde_derive v1.0.219\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m smallvec v1.15.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m zerocopy v0.8.25\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m pkg-config v0.3.32\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m zstd-sys v2.0.15+zstd.1.5.7\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-epoch v0.9.18\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ggml-sys v0.1.0 (/root/.cargo/git/checkouts/llama-rs-2a58aa62f75fd715/6e8aa79/ggml-sys)\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m aho-corasick v1.1.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rayon-core v1.12.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m utf8parse v0.2.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m regex-syntax v0.8.5\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m autocfg v1.4.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m syn v1.0.109\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rustversion v1.0.20\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m hashbrown v0.7.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m regex-automata v0.4.9\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-deque v0.8.6\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m getrandom v0.2.16\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m tinyvec_macros v0.1.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rustix v0.38.44\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m thiserror v1.0.69\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m serde_json v1.0.140\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m itoa v1.0.15\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ryu v1.0.20\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m object v0.36.7\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m protobuf v2.14.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m either v1.15.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m tinyvec v1.9.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rand_core v0.6.4\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ppv-lite86 v0.2.21\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m regex v1.11.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m anstyle-parse v0.2.6\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m thiserror-impl v1.0.69\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m csv-core v0.1.12\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m zstd-safe v6.0.6\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m anstyle v1.0.10\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m eyre v0.6.12\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rust_tokenizers v3.1.6\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m lazy_static v1.5.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m gimli v0.31.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m adler2 v2.0.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ahash v0.3.8\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m is_terminal_polyfill v1.70.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m bitflags v2.9.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m anstyle-query v1.1.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m owo-colors v4.2.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m linux-raw-sys v0.4.15\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m colorchoice v1.0.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m heck v0.4.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m strum_macros v0.24.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m anstream v0.6.18\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m miniz_oxide v0.8.8\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m addr2line v0.24.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m csv v1.3.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ggml v0.1.0 (/root/.cargo/git/checkouts/llama-rs-2a58aa62f75fd715/6e8aa79/ggml)\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rayon v1.10.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rand_chacha v0.3.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-normalization v0.1.24\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m itertools v0.8.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m nibble_vec v0.1.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-normalization-alignments v0.1.12\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m dirs-sys-next v0.1.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m heck v0.5.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m endian-type v0.1.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m log v0.4.27\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m strsim v0.11.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m once_cell v1.21.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m bitflags v1.3.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rustc-demangle v0.1.24\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m clap_lex v0.7.4\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m indenter v0.3.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m clap_builder v4.5.37\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m backtrace v0.3.75\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m nix v0.26.4\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m radix_trie v0.2.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m clap_derive v4.5.32\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m dirs-next v2.0.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rand v0.8.5\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ggml-format v0.1.0 (/root/.cargo/git/checkouts/llama-rs-2a58aa62f75fd715/6e8aa79/ggml-format)\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m fd-lock v3.0.13\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m strum v0.24.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m serde_bytes v0.11.17\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m half v2.6.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m memmap2 v0.5.10\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m is-terminal v0.4.16\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m maplit v1.0.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m partial_sort v0.2.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-width v0.1.14\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m termcolor v1.4.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m bytemuck v1.23.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m humantime v2.2.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-segmentation v1.12.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rustyline v11.0.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m env_logger v0.10.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m llama-rs v0.1.0 (/root/.cargo/git/checkouts/llama-rs-2a58aa62f75fd715/6e8aa79/llama-rs)\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m spinners v4.1.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m color-eyre v0.6.4\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m zstd v0.12.4\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m clap v4.5.37\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m bincode v1.3.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m num_cpus v1.16.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m llama-cli v0.1.0 (/root/.cargo/git/checkouts/llama-rs-2a58aa62f75fd715/6e8aa79/llama-cli)\n",
            "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 4m 20s\n",
            "\u001b[1m\u001b[32m  Installing\u001b[0m /root/.cargo/bin/llama-cli\n",
            "\u001b[1m\u001b[32m   Installed\u001b[0m package `llama-cli v0.1.0 (https://github.com/dustletter/llama-rs#6e8aa794)` (executable `llama-cli`)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/rustformers/llm/tree/main"
      ],
      "metadata": {
        "id": "UsxFyC6_amcn"
      }
    },
    {
      "source": [
        "!llama-cli --help"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp0a5ktjaewY",
        "outputId": "706d40c3-107d-46f8-d917-1502adedc1be"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[4mUsage:\u001b[0m \u001b[1mllama-cli\u001b[0m <COMMAND>\n",
            "\n",
            "\u001b[1m\u001b[4mCommands:\u001b[0m\n",
            "  \u001b[1minfer\u001b[0m              Use a model to infer the next tokens in a sequence, and exit\n",
            "  \u001b[1mdump-tokens\u001b[0m        Dumps the prompt to console and exits, first as a comma-separated list of token IDs and then as a list of comma-separated string keys and token ID values\n",
            "  \u001b[1mrepl\u001b[0m               Use a model to interactively prompt it multiple times, while resetting the context between invocations\n",
            "  \u001b[1mchat-experimental\u001b[0m  Use a model to interactively generate tokens, and chat with it\n",
            "  \u001b[1mquantize\u001b[0m           Quantize a GGML model to 4-bit\n",
            "  \u001b[1mhelp\u001b[0m               Print this message or the help of the given subcommand(s)\n",
            "\n",
            "\u001b[1m\u001b[4mOptions:\u001b[0m\n",
            "  \u001b[1m-h\u001b[0m, \u001b[1m--help\u001b[0m     Print help\n",
            "  \u001b[1m-V\u001b[0m, \u001b[1m--version\u001b[0m  Print version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "يستخدم بالفعل Rust لتشغيل النموذج.\n",
        "\n",
        "شرح:\n",
        "\n",
        "llama-cli: هذه الأداة المساعدة مبنية بلغة Rust ويتم استخدامها للتفاعل مع نماذج اللغة.\n",
        "infer: هذا هو الأمر الفرعي الذي يطلب من llama-cli إجراء استنتاج على النموذج. بمعنى آخر، فإنه يطلب من النموذج إنشاء نص بناءً على المدخلات المقدمة.\n",
        "-m /content/ggml-alpaca-7b-q4.bin: يحدد مسار النموذج المراد استخدامه.\n",
        "-p \"Tell me how cool the Rust programming language is:\": يحدد النص المراد إدخاله إلى النموذج.\n",
        "لماذا Rust؟"
      ],
      "metadata": {
        "id": "VUke6g0nbbC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "نعم، تم استخدام Rust في بناء llama-cli التي تستخدم لتشغيل نموذج اللغة المحدد في الأمر."
      ],
      "metadata": {
        "id": "9lfgEeh6b0Ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!llama-cli infer -m /content/ggml-alpaca-7b-q4.bin -p \"Tell me how cool the Rust programming language is:\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgzQ6visawk5",
        "outputId": "d14102c7-f49d-4561-b05b-f5f117370c5a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:37Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loading model part 1/1 from '/content/ggml-alpaca-7b-q4.bin' (mmap preferred: true)\n",
            "    \n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:37Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m ggml ctx size = 4017.32 MB\n",
            "    \n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:38Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 8/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:40Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 16/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:41Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 24/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:42Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 32/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:42Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 40/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:42Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 48/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:43Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 56/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:43Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 64/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:43Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 72/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:44Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 80/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:44Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 88/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:45Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 96/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:46Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 104/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:47Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 112/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:48Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 120/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:49Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 128/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:50Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 136/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:50Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 144/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:51Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 152/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:52Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 160/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:53Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 168/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:54Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 176/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:55Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 184/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:56Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 192/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:57Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 200/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:58Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 208/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:09:59Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 216/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:10:00Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 224/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:10:01Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 232/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:10:02Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 240/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:10:03Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 248/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:10:03Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 256/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:10:04Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 264/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:10:05Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 272/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:10:05Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 280/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:10:06Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 288/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:10:07Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loading of '/content/ggml-alpaca-7b-q4.bin' complete\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:10:07Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Model size = 0.00 MB / num tensors = 291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:10:07Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Model fully loaded! Elapsed: 29485ms\n",
            "Tell me how cool the Rust programming language is:\n",
            "Roust Programming Language Overview - Wakoopa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/dustletter/llama-rs?tab=readme-ov-file"
      ],
      "metadata": {
        "id": "Irq3-NPdbka4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!llama-cli infer -m /content/Llama-3.2-3B-Instruct-IQ3_M.gguf -p \"Tell me how cool the Rust programming language is:\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9v35WQpcYRp",
        "outputId": "4e4f224e-3fab-49f2-b013-3926ee64121c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:12:57Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loading model part 1/1 from '/content/Llama-3.2-3B-Instruct-IQ3_M.gguf' (mmap preferred: true)\n",
            "    \n",
            "Error: \n",
            "   0: \u001b[91mCould not load model\u001b[0m\n",
            "   1: \u001b[91minvalid magic number for \"/content/Llama-3.2-3B-Instruct-IQ3_M.gguf\"\u001b[0m\n",
            "\n",
            "Backtrace omitted. Run with RUST_BACKTRACE=1 environment variable to display it.\n",
            "Run with RUST_BACKTRACE=full to include source snippets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/ggml-alpaca-7b-q4.bin\n"
      ],
      "metadata": {
        "id": "Q0A8FaKfcdLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llama-cli infer -m /content/ggml-alpaca-7b-q4.bin -p \"Who is Napoleon Bonaparte?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ2Y8lJ3clCg",
        "outputId": "376692ec-9f6d-4c75-8ede-3340aa337282"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:25Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loading model part 1/1 from '/content/ggml-alpaca-7b-q4.bin' (mmap preferred: true)\n",
            "    \n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:25Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m ggml ctx size = 4017.32 MB\n",
            "    \n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:26Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 8/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:26Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 16/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:26Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 24/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:26Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 32/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:26Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 40/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:26Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 48/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:26Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 56/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:26Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 64/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:26Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 72/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:26Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 80/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:26Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 88/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:27Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 96/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:27Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 104/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:28Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 112/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:29Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 120/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:30Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 128/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:30Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 136/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:31Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 144/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:32Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 152/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:33Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 160/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:33Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 168/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:34Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 176/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:35Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 184/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:36Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 192/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:36Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 200/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:37Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 208/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:38Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 216/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:39Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 224/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:39Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 232/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:40Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 240/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:41Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 248/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:43Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 256/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:44Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 264/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:45Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 272/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:46Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 280/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:47Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loaded tensor 288/291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:47Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Loading of '/content/ggml-alpaca-7b-q4.bin' complete\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:47Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Model size = 0.00 MB / num tensors = 291\n",
            "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-05-06T21:14:47Z \u001b[0m\u001b[32mINFO \u001b[0m llama_cli::cli_args\u001b[0m\u001b[38;5;8m]\u001b[0m Model fully loaded! Elapsed: 22039ms\n",
            "Who is Napoleon Bonaparte?\n",
            "Napoleon Bonaparte was a French military leader and emperor who rose to power during the Napoleonic Wars. He established himself as an important figure in European history, leading France’s army from 1799 until his downfall at Waterloo in 1815. As one of the most influential figures of the early 19th century, Napoleon is remembered for modernizing much of Europe and creating many of the French legal codes still used today. He was also an author of several books on a variety of subjects during his lifetime as well as an art collector.\n",
            "What were some major accomplishments achieved by Napoleon Bonaparte?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Im2NJKCpcyzw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}